{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel(\"Continual_Learning/G1.xlsx\", index_col=0)\n",
    "df2 = pd.read_excel(\"Continual_Learning/G2.xlsx\", index_col=0)\n",
    "df3 = pd.read_excel(\"Continual_Learning/G3.xlsx\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping any rows with NaN values\n",
    "df1 = df1.dropna()\n",
    "df2 = df2.dropna()\n",
    "df3 = df3.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[[\"tags\", \"text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating the following tagging scheme for the NER task:\n",
    "\n",
    "\n",
    "| Entity_name | Token |\n",
    "| --- | --- |\n",
    "| Other | 0 |\n",
    "| treatment | 1 |\n",
    "| chronic_disease | 2 |\n",
    "| cancer | 3 |\n",
    "| allergy_name | 4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ids = {\n",
    "    \"treatment\": 1,\n",
    "    \"chronic_disease\": 2,\n",
    "    \"cancer\": 3,\n",
    "    \"allergy_name\": 4,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_index(txt, word):\n",
    "    pattern = re.compile(r'\\b{}\\b'.format(re.escape(word)))\n",
    "\n",
    "    # Find the index of the element containing the pattern\n",
    "    word_index = next((index for index, element in enumerate(txt) if pattern.search(element)), None)\n",
    "\n",
    "    return word_index\n",
    "\n",
    "def get_ner_tokens(row):\n",
    "\n",
    "    # Few tags have leading and trailing commas, removing them\n",
    "    tag = row.tags.strip(\",\").strip()    # start:end:name, start:end:name, ... (start and end are in character level)\n",
    "\n",
    "    # removing leading and trailing whitespace\n",
    "    txt = row.text\n",
    "\n",
    "\n",
    "    try:\n",
    "        # txt = txt.split()\n",
    "        original_txt = txt\n",
    "        txt = word_tokenize(row[\"text\"])\n",
    "    except:\n",
    "        # print(tag, txt)\n",
    "        return None, None\n",
    "\n",
    "    \n",
    "\n",
    "    # labeled every word as other\n",
    "    labels = np.zeros(len(txt))\n",
    "\n",
    "    # iterate over all tages and mark them with their token\n",
    "    for t in tag.split(\",\"):\n",
    "        if t == \"\":\n",
    "            continue\n",
    "        start, end, name = t.split(\":\")\n",
    "\n",
    "        # as first character is considered as 1 in the dataset, but in python it is 0\n",
    "        start, end = int(start), int(end)\n",
    "        start -= 1\n",
    "        end -= 1\n",
    "\n",
    "        exact_word = original_txt[start:end]\n",
    "\n",
    "        n_exact_words = len(exact_word.split())\n",
    "\n",
    "        # check if word is more than one word, if yes then get the index of the first word and save total number of words\n",
    "        if n_exact_words > 1:\n",
    "\n",
    "            exact_word = exact_word.split()[0]\n",
    "\n",
    "            word_index = find_word_index(txt, exact_word)\n",
    "            # word_index = txt.index(exact_word)\n",
    "            try:\n",
    "                for i in range(word_index, word_index+n_exact_words):\n",
    "                    labels[i] = entity_ids[name]\n",
    "            except:\n",
    "                # print(txt, exact_word, word_index, n_exact_words)\n",
    "                return None, None\n",
    "\n",
    "        else:\n",
    "            \n",
    "\n",
    "            word_index = find_word_index(txt, exact_word)\n",
    "\n",
    "            labels[word_index] = entity_ids[name]\n",
    "\n",
    "    return txt, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"tokens\"], df1[\"ner_tags\"] = zip(*df1.apply(get_ner_tokens, axis=1))\n",
    "df2[\"tokens\"], df2[\"ner_tags\"] = zip(*df2.apply(get_ner_tokens, axis=1))\n",
    "df3[\"tokens\"], df3[\"ner_tags\"] = zip(*df3.apply(get_ner_tokens, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.iloc[1177]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, tags  =zip(*pd.DataFrame(df1.iloc[1177]).T.apply(get_ner_tokens, axis=1))\n",
    "tokens, tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with None values\n",
    "df1.dropna(inplace=True)\n",
    "df2.dropna(inplace=True)\n",
    "df3.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"tokens\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"ner_tags\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to array for labels and tokens\n",
    "df1[\"tokens\"] = df1[\"tokens\"].apply(np.array)\n",
    "df2[\"tokens\"] = df2[\"tokens\"].apply(np.array)\n",
    "df3[\"tokens\"] = df3[\"tokens\"].apply(np.array)\n",
    "\n",
    "df1[\"ner_tags\"] = df1[\"ner_tags\"].apply(np.array)\n",
    "df2[\"ner_tags\"] = df2[\"ner_tags\"].apply(np.array)\n",
    "df3[\"ner_tags\"] = df3[\"ner_tags\"].apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"processed_data/G1.csv\", index=False)\n",
    "df2.to_csv(\"processed_data/G2.csv\", index=False)\n",
    "df3.to_csv(\"processed_data/G3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv(\"processed_data/G1.csv\")\n",
    "# df2 = pd.read_csv(\"processed_data/G2.csv\")\n",
    "# df3 = pd.read_csv(\"processed_data/G3.csv\")\n",
    "\n",
    "# add new feature dataset_id\n",
    "df1[\"dataset_num\"] = 1\n",
    "df2[\"dataset_num\"] = 2\n",
    "df3[\"dataset_num\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"tokens\"].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"ner_tags\"].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_cols(df):\n",
    "    df.rename(columns={\"ID\": \"id\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "df1 = rename_cols(df1)\n",
    "df2 = rename_cols(df2)\n",
    "df3 = rename_cols(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = pd.concat([df1, df2, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset.dataset_num.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data to dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset = {\n",
    "    \"id\": custom_dataset[\"id\"],\n",
    "    \"tags\": custom_dataset[\"tags\"],\n",
    "    \"text\": custom_dataset[\"text\"],\n",
    "    \"dataset_num\" : custom_dataset[\"dataset_num\"],\n",
    "    \"tokens\": custom_dataset[\"tokens\"],\n",
    "    \"ner_tags\": custom_dataset[\"ner_tags\"],\n",
    "}\n",
    "\n",
    "# Create a Hugging Face Dataset object\n",
    "dataset = Dataset.from_dict(huggingface_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict#, train_test_split\n",
    "\n",
    "# Assuming your original DatasetDict is called original_dataset\n",
    "original_train_data = dataset\n",
    "\n",
    "# Get unique dataset_num values\n",
    "unique_dataset_nums = [1,2,3]#original_train_data[\"dataset_num\"].unique()\n",
    "\n",
    "# Initialize empty datasets for train and test\n",
    "combined_data = {\"train\": [], \"test\": []}\n",
    "\n",
    "# Split each dataset_num into train and test\n",
    "for dataset_num in unique_dataset_nums:\n",
    "    subset_data = original_train_data.filter(lambda example: example[\"dataset_num\"] == dataset_num)\n",
    "\n",
    "    print(subset_data)\n",
    "    # Split the subset into train and test using datasets.train_test_split\n",
    "    splited_subset = subset_data.train_test_split( test_size=0.2, seed=42)\n",
    "\n",
    "    # merged the respective train and test data to the combined_data\n",
    "    combined_data[\"train\"].append(splited_subset[\"train\"])\n",
    "    combined_data[\"test\"].append(splited_subset[\"test\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Concatenate all train data\n",
    "combined_data[\"train\"] = concatenate_datasets(combined_data[\"train\"])\n",
    "combined_data[\"test\"] = concatenate_datasets(combined_data[\"test\"])\n",
    "\n",
    "# combined_data = DatasetDict({\"train\": train_data, \"test\": test_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = DatasetDict(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.save_to_disk(\"custom_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing the dataset to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if 'kaggle_web_client' in sys.modules:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    HUGGINGFACE_API_KEY = user_secrets.get_secret(\"HUGGINGFACE_API_KEY\")\n",
    "elif 'google.colab' in sys.modules:\n",
    "    !pip -q install python-dotenv\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "else:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from huggingface_hub import login\n",
    "login(token=HUGGINGFACE_API_KEY, write_permission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.push_to_hub(\"SKT27182/NER_processed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\"name\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict[\"name\"].extend([\"saurabh\", \"ramesh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
